{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39dda0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Read your notebook (assuming version 4 for example purposes)\n",
    "nb = nbformat.read(\"ml_project.ipynb\", as_version=4)\n",
    "\n",
    "# Normalize the notebook to add missing id fields and other updates\n",
    "nbformat.validator.validate(nb)\n",
    "\n",
    "# Write the normalized notebook back to a file\n",
    "nbformat.write(nb, \"ml_project_normalized.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39cd719a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in /u/nneoma/.local/lib/python3.8/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /u/nneoma/.local/lib/python3.8/site-packages (from xgboost) (1.24.4)\n",
      "Requirement already satisfied: scipy in /u/nneoma/.local/lib/python3.8/site-packages (from xgboost) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nneoma/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique primary colors:\n",
      " 1. black\n",
      " 2. non-black\n",
      "dropped columns\n",
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "Unique primary colors:\n",
      " 1. black\n",
      " 2. non-black\n",
      "cleaned color\n",
      "cleaned breed\n",
      "dropped columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1020677/1199118304.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt_series = pd.to_datetime(df['intake_time'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "Unique primary colors:\n",
      " 1. black\n",
      " 2. non-black\n",
      "cleaned color\n",
      "cleaned breed\n",
      "Done running ml_project.ipynb.\n"
     ]
    }
   ],
   "source": [
    "%run ml_project_normalized.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b7e98aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, LabelEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "#from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c4fe6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def threshold_adjusted_cross_val(X_train, y_train, model, thresholds, skf):\n",
    "    \"\"\"\n",
    "    Perform cross-validation with threshold adjustment after predicting probabilities.\n",
    "    \"\"\"\n",
    "    # Store the balanced accuracy scores for each fold\n",
    "    balanced_accuracy_scores = []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_train, y_train):\n",
    "        # Split the data into training and validation sets\n",
    "        X_train_fold, X_val_fold = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        # Fit the model on the training fold\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        # Get predicted probabilities for the validation set\n",
    "        probs = model.predict_proba(X_val_fold)\n",
    "        \n",
    "        # Adjust predictions based on the thresholds\n",
    "        test_preds_adjusted = []\n",
    "\n",
    "        for i, prob in enumerate(test_probs):\n",
    "            # Get classes over their threshold\n",
    "            passed = [cls for cls, p in enumerate(prob) if p >= thresholds.get(cls, 1.0)]\n",
    "            if passed:\n",
    "                # Pick the one with the highest probability\n",
    "                best_class = max(passed, key=lambda cls: prob[cls])\n",
    "            else:\n",
    "                # Fallback to class with highest probability (no thresholds passed)\n",
    "                best_class = np.argmax(prob)\n",
    "            test_preds_adjusted.append(best_class)\n",
    "\n",
    "        test_preds_adjusted = np.array(test_preds_adjusted)\n",
    "        \n",
    "        # Calculate balanced accuracy for the validation set\n",
    "        balanced_acc = balanced_accuracy_score(y_val_fold, y_pred_adjusted)\n",
    "        balanced_accuracy_scores.append(balanced_acc)\n",
    "    \n",
    "    # Return the mean balanced accuracy across all folds\n",
    "    return balanced_accuracy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a20c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'logreg__penalty': 'l2', 'logreg__fit_intercept': False, 'logreg__class_weight': 'balanced', 'logreg__C': 0.01}\n",
    "# Best parameters: {'logreg__solver': 'lbfgs', 'logreg__penalty': 'l2', 'logreg__max_iter': 5000, 'logreg__fit_intercept': True, 'logreg__class_weight': {0: 0.4040806125462931, 1: 21.29916167664671, 2: 6.4625, 3: 1.3310979717087044, 4: 0.6360118728319565}, 'logreg__C': 0.01}\n",
    "\n",
    "def train_logreg_classifier(X_train, y_train, X_test, cat_cols, num_cols):\n",
    " #   cat_cols = ['intake_type', 'intake_condition', 'animal_type', 'sex_upon_intake', 'breed','primary_color','season', 'is_mix']\n",
    " #   num_cols = ['age_upon_intake', 'intake_year']    # Replace with your actual numerical columns\n",
    "\n",
    "  #  cat_cols = [0, 1, 2, 3, 5,7,8, 10]\n",
    "  #  num_cols = [4, 6] \n",
    "\n",
    "    \n",
    "\n",
    "    all_cols = X_train.columns.tolist()\n",
    "\n",
    "    # Convert column names to positional indices\n",
    "    cat_indices = [all_cols.index(col) for col in cat_cols]\n",
    "    num_indices = [all_cols.index(col) for col in num_cols]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    class_labels = np.unique(y_train)  # Get unique class labels in your target variable\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=class_labels,\n",
    "        y=y_train\n",
    "    )\n",
    "    class_weight_dict = dict(zip(class_labels, class_weights))  # Map class labels to weights\n",
    "\n",
    "    # Handle rare categories\n",
    "    rare_threshold = 0.01\n",
    "    for col in cat_cols:\n",
    "        freq = X_train[col].value_counts(normalize=True)\n",
    "        rare_categories = freq[freq < rare_threshold].index\n",
    "        X_train[col] = X_train[col].apply(lambda x: 'Other' if x in rare_categories else x)\n",
    "        X_test[col] = X_test[col].apply(lambda x: 'Other' if x in rare_categories else x)\n",
    "\n",
    "    for col in cat_cols:\n",
    "        X_train[col] = X_train[col].astype(str)\n",
    "        X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "\n",
    "    sampling_strategy = {\n",
    "        0: 55044,  # Adopted: Keep it as it is (no oversampling)\n",
    "        1: 25000,  # Euth: Oversample 70% of the existing samples\n",
    "        2: 10000,  # Died\n",
    "        3: 30000,  # RTO\n",
    "        4: 35211,  # Transfer\n",
    "    }\n",
    "\n",
    "\n",
    "    # Preprocessing for numerical features: Imputation (if needed) + Standardization\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "    #    ('imputer', SimpleImputer(strategy='median')),  # Impute missing values (if any)\n",
    "        ('scaler', StandardScaler())  # Standardize numerical features\n",
    "    ]) # i added mean=False just bc\n",
    "\n",
    "    # Preprocessing for categorical features: Imputation (if needed) + One-Hot Encoding\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "     #   ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values (if any)\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-Hot Encode categorical features\n",
    "    ])\n",
    "\n",
    "    # Combine both transformations into a single ColumnTransformer\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, num_indices),\n",
    "            ('cat', categorical_transformer, cat_indices)\n",
    "        ])\n",
    "\n",
    "    # Create a pipeline that first transforms the data and then applies Logistic Regression\n",
    "    pipeline = Pipeline(steps=[\n",
    "      #  ('smote', SMOTENC(categorical_features=cat_cols, random_state=42, sampling_strategy=sampling_strategy)),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('logreg', LogisticRegression())  # You can adjust max_iter as needed\n",
    "    ])\n",
    "\n",
    "    param_distributions = {\n",
    "        \"logreg__C\": uniform(0.01, 1), #uniform(loc=0.01, scale=10),\n",
    "        \"logreg__penalty\": ['l2'],\n",
    "        'logreg__solver' : ['lbfgs','saga'],\n",
    "        \"logreg__fit_intercept\": [True, False],\n",
    "        'logreg__class_weight': [class_weight_dict, 'balanced'],\n",
    "        'logreg__max_iter' : [7000, 7500, 8000]\n",
    "    }\n",
    "\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=10,\n",
    "        cv=skf,\n",
    "        scoring=balanced_acc_scorer,\n",
    "        verbose=3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    randomized_search.fit(X_train, y_train)\n",
    "\n",
    "    print('Best parameters:', randomized_search.best_params_)\n",
    "    print('Best cross-validation accuracy:', randomized_search.best_score_)\n",
    "\n",
    "    # Define thresholds for each class (adjust based on your needs)\n",
    "    thresholds = {\n",
    "        0: 0.90,  # For Adopted\n",
    "        1: 0.10,   # For Died\n",
    "        2: 0.20,   # For Euth\n",
    "        3: 0.30,  # For RTO\n",
    "        4: 0.30    # For Transfer\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "    cv_scores = cross_val_score(randomized_search.best_estimator_, X_train, y_train, cv=5, verbose=3, scoring=balanced_acc_scorer)\n",
    "    print('Generalization Balanced accuracy (via cross_val_score):', cv_scores.mean())\n",
    "\n",
    "    test_predictions = randomized_search.predict(X_test)\n",
    "    '''\n",
    "    cv_thresh_scores = threshold_adjusted_cross_val(X_train, y_train, randomized_search.best_estimator_, thresholds, skf)\n",
    "    print('Generalization Balanced accuracy (via cross_val_score):', np.mean(cv_thresh_scores))\n",
    "\n",
    "        # Adjust the test predictions using the thresholds\n",
    "    test_probs = randomized_search.predict_proba(X_test)\n",
    "    test_preds_adjusted = []\n",
    "\n",
    "    for i, prob in enumerate(test_probs):\n",
    "        # Get classes over their threshold\n",
    "        passed = [cls for cls, p in enumerate(prob) if p >= thresholds.get(cls, 1.0)]\n",
    "        if passed:\n",
    "            # Pick the one with the highest probability\n",
    "            best_class = max(passed, key=lambda cls: prob[cls])\n",
    "        else:\n",
    "            # Fallback to class with highest probability (no thresholds passed)\n",
    "            best_class = np.argmax(prob)\n",
    "        test_preds_adjusted.append(best_class)\n",
    "\n",
    "    test_preds_adjusted = np.array(test_preds_adjusted)\n",
    "    \n",
    "    coefs = randomized_search.best_estimator_.coef_\n",
    "    importance_df = pd.DataFrame(coefs.T, columns= randomized_search.best_estimator_.classes_, index= X_train.columns)\n",
    "    '''\n",
    "\n",
    "    return randomized_search.best_estimator_, test_predictions#, importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ac4760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thresholding function - need tadjust in CV later\n",
    "def apply_thresholds(y_proba, thresholds):\n",
    "    preds = []\n",
    "    for prob in y_proba:\n",
    "        # Pick classes where prob >= threshold\n",
    "        candidates = [i for i, p in enumerate(prob) if p >= thresholds[i]]\n",
    "        if candidates:\n",
    "            pred = max(candidates, key=lambda i: prob[i])\n",
    "        else:\n",
    "            pred = np.argmax(prob)\n",
    "        preds.append(pred)\n",
    "    return np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69018b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# does log of age and removes age\n",
    "def log_age(df):\n",
    "    df['log_age'] = np.log1p(df['age_upon_intake'])\n",
    "    df = df.drop(columns=['age_upon_intake'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04f54764",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = bucket_seasons(df_train)\n",
    "df_test = bucket_seasons(df_test)\n",
    "\n",
    "df_train = bucket_days(df_train)\n",
    "df_test = bucket_days(df_test)\n",
    "\n",
    "df_train['breed'] = df_train['breed'].astype(str)\n",
    "df_test['breed'] = df_test['breed'].astype(str)\n",
    "\n",
    "df_train = df_train.drop(columns=['size', 'intake_hour', 'intake_month'])\n",
    "df_test = df_test.drop(columns=['size', 'intake_hour', 'intake_month'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05f1833a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intake_type</th>\n",
       "      <th>intake_condition</th>\n",
       "      <th>animal_type</th>\n",
       "      <th>sex_upon_intake</th>\n",
       "      <th>breed</th>\n",
       "      <th>intake_year</th>\n",
       "      <th>primary_color</th>\n",
       "      <th>is_mix</th>\n",
       "      <th>outcome_type</th>\n",
       "      <th>season</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>log_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stray</td>\n",
       "      <td>Normal / Behavior</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Spayed Female</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>non-black</td>\n",
       "      <td>0</td>\n",
       "      <td>Return to Owner</td>\n",
       "      <td>Summer</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>4.574711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stray</td>\n",
       "      <td>Normal / Behavior</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Intact Male</td>\n",
       "      <td>7</td>\n",
       "      <td>2016</td>\n",
       "      <td>non-black</td>\n",
       "      <td>1</td>\n",
       "      <td>Return to Owner</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Evening</td>\n",
       "      <td>2.484907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Public Assist</td>\n",
       "      <td>Normal / Behavior</td>\n",
       "      <td>Cat</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>Common</td>\n",
       "      <td>2022</td>\n",
       "      <td>non-black</td>\n",
       "      <td>0</td>\n",
       "      <td>Transfer</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Night</td>\n",
       "      <td>3.218876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Owner Surrender</td>\n",
       "      <td>Normal / Behavior</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>2</td>\n",
       "      <td>2017</td>\n",
       "      <td>non-black</td>\n",
       "      <td>1</td>\n",
       "      <td>Return to Owner</td>\n",
       "      <td>Winter</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>3.218876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Public Assist</td>\n",
       "      <td>Normal / Behavior</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Neutered Male</td>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>black</td>\n",
       "      <td>1</td>\n",
       "      <td>Return to Owner</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Morning</td>\n",
       "      <td>4.290459</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       intake_type   intake_condition animal_type sex_upon_intake   breed  \\\n",
       "0            Stray  Normal / Behavior         Dog   Spayed Female       2   \n",
       "1            Stray  Normal / Behavior         Dog     Intact Male       7   \n",
       "2    Public Assist  Normal / Behavior         Cat   Neutered Male  Common   \n",
       "3  Owner Surrender  Normal / Behavior         Dog   Neutered Male       2   \n",
       "4    Public Assist  Normal / Behavior         Dog   Neutered Male       3   \n",
       "\n",
       "   intake_year primary_color  is_mix     outcome_type  season time_of_day  \\\n",
       "0         2015     non-black       0  Return to Owner  Summer   Afternoon   \n",
       "1         2016     non-black       1  Return to Owner  Spring     Evening   \n",
       "2         2022     non-black       0         Transfer  Spring       Night   \n",
       "3         2017     non-black       1  Return to Owner  Winter   Afternoon   \n",
       "4         2019         black       1  Return to Owner  Spring     Morning   \n",
       "\n",
       "    log_age  \n",
       "0  4.574711  \n",
       "1  2.484907  \n",
       "2  3.218876  \n",
       "3  3.218876  \n",
       "4  4.290459  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = log_age(df_train)\n",
    "df_test = log_age(df_test)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36da7a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['intake_type', 'intake_condition', 'animal_type', 'sex_upon_intake', 'breed','season', 'is_mix', 'time_of_day', 'primary_color'] # experimenting with droping color\n",
    "num_cols = ['log_age', 'intake_year']    # Replace with your actual numerical columns\n",
    "# freq_cols = ['primary_color'] # for trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3f1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2cae7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=['outcome_type'])\n",
    "y = df_train['outcome_type']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989e9b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding mapping: ['Adoption' 'Died' 'Euthanasia' 'Return to Owner' 'Transfer']\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/nneoma/.local/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5] END logreg__C=0.3845401188473625, logreg__class_weight={0: 0.4040806125462931, 1: 21.29916167664671, 2: 6.4625, 3: 1.3310979717087044, 4: 0.6360118728319565}, logreg__fit_intercept=True, logreg__max_iter=8000, logreg__penalty=l2, logreg__solver=saga;, score=0.372 total time= 2.5min\n"
     ]
    }
   ],
   "source": [
    "# Encode the target variable.\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "print('Encoding mapping:', le.classes_)\n",
    "\n",
    "'''\n",
    "# Identify rare classes that need oversampling.\n",
    "rare_classes = [\n",
    "    label for label, count in pd.Series(y_train).value_counts().items()\n",
    "    if count < 0.05 * len(y_train)\n",
    "]\n",
    "print(\"Rare classes:\")\n",
    "for cls in rare_classes:\n",
    "    print(f\"  {cls}: {le.classes_[cls]}\")\n",
    "\n",
    "\n",
    "# Define column groups (this example will one-hot encode all columns, so cat_cols and num_cols are not used in the transformer).\n",
    "cat_cols = {'intake_type', 'intake_condition', 'animal_type', 'sex_upon_intake', 'breed', 'intake_hour', 'intake_year', 'primary_color', 'season', 'is_mix', 'age_group'}\n",
    "categorical_features = [col for col in X_train.columns if col in cat_cols]\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "best_model, test_predictions = train_logreg_classifier(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    cat_cols=cat_cols,\n",
    "    num_cols=num_cols\n",
    "\n",
    ")\n",
    "\n",
    "# Get predicted probabilities from the best model\n",
    "#y_val_proba = best_model.predict_proba(X_test)\n",
    "\n",
    "# Define per-class thresholds (example: tune these)\n",
    "# Order: [Adoption, Died, Euthanasia, RTO, Transfer]\n",
    "thresholds = [0.6, 0.3, 0.3, 0.4, 0.4]\n",
    "\n",
    "\n",
    "# Apply thresholds to the predicted probabilities\n",
    "# thresholded_preds = apply_thresholds(y_val_proba, thresholds)\n",
    "\n",
    "# Convert predictions back to original labels.\n",
    "classification_report_with_accuracy_score(y_test, test_predictions)\n",
    "#print(importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced54280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging attempt\\\n",
    "# bagging logreg\n",
    "def train_bagging_classifier(X_train, y_train, X_test, base_model, n_estimators=50):\n",
    "    \"\"\"\n",
    "    Trains a Bagging Classifier with the given base model and evaluates it on the test data.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_train: Training features\n",
    "    - y_train: Training labels\n",
    "    - X_test: Test features\n",
    "    - base_model: The base model to use for the bagging (e.g., Logistic Regression)\n",
    "    - n_estimators: Number of base models in the ensemble (default is 50)\n",
    "    \n",
    "    Returns:\n",
    "    - trained_model: Trained BaggingClassifier\n",
    "    - test_predictions: Predictions on the test set\n",
    "    \"\"\"\n",
    "    # Create the BaggingClassifier with the base model\n",
    "    bagging_clf = BaggingClassifier(base_estimator=base_model, n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Fit the model\n",
    "    bagging_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    test_predictions = bagging_clf.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    training_accuracy = balanced_accuracy_score(y_train, bagging_clf.predict(X_train))\n",
    "    print(f\"Training Accuracy: {training_accuracy:.4f}\")\n",
    "\n",
    "    # needs CV if not a flop\n",
    "    \n",
    "    return bagging_clf, test_predictions\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Create a base Logistic Regression model\n",
    "logreg, test_predict= train_logreg_classifier(X_train=X_train, y_train=y_train, X_test=X_test,)\n",
    "\n",
    "bagging_model, test_predictions = train_bagging_classifier(X_train, y_train, X_test, base_model=logreg, n_estimators=50)\n",
    "\n",
    "classification_report_with_accuracy_score(y_test, test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
