{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9c57ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "dropped columns\n",
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "cleaned breed\n",
      "cleaned color\n",
      "dropped columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zg/knpz54sj16988z9vrp2b98fr0000gn/T/ipykernel_53799/4020467989.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt_series = pd.to_datetime(df['intake_time'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "cleaned breed\n",
      "cleaned color\n",
      "Done running ml_project.ipynb.\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "# Read your notebook (assuming version 4 for example purposes)\n",
    "nb = nbformat.read(\"ml_project.ipynb\", as_version=4)\n",
    "\n",
    "# Normalize the notebook to add missing id fields and other updates\n",
    "nbformat.validator.validate(nb)\n",
    "\n",
    "# Write the normalized notebook back to a file\n",
    "nbformat.write(nb, \"ml_project_normalized.ipynb\")\n",
    "\n",
    "%run ml_project_normalized.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a10e5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_classifier(X_train, y_train, X_test, rare_classes, target_samples_per_class=3000):\n",
    "  \"\"\"\n",
    "  Trains an Extremely Randomized Trees model using class weighting and RandomizedSearchCV.\n",
    "\n",
    "  Parameters:\n",
    "    X_train (pd.DataFrame or np.ndarray): Fully preprocessed training features.\n",
    "    y_train (array-like): Encoded training labels.\n",
    "    X_test (pd.DataFrame or np.ndarray): Fully preprocessed test features.\n",
    "    rare_classes (list): List of int-encoded rare class labels.\n",
    "    target_samples_per_class (int): Target number of samples per rare class (currently unused).\n",
    "\n",
    "  Returns:\n",
    "    best_estimator: Trained model.\n",
    "    test_predictions: Predictions on the test set.\n",
    "  \"\"\"\n",
    "\n",
    "  # Compute class weights for imbalanced training set\n",
    "  class_labels = np.unique(y_train)\n",
    "  class_weights = compute_class_weight('balanced', classes=class_labels, y=y_train)\n",
    "  class_weight_dict = dict(zip(class_labels, class_weights))\n",
    "  print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "  # Build model pipeline (only model step, encoding was done externally)\n",
    "  model = ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    "  )\n",
    "\n",
    "  # Define hyperparameter search space\n",
    "  param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "  }\n",
    "\n",
    "  search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=5,\n",
    "    cv=5,\n",
    "    scoring='balanced_accuracy',\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=2\n",
    "  )\n",
    "\n",
    "  print(f\"\\n[INFO] Starting training with {len(X_train)} samples\")\n",
    "  search.fit(X_train, y_train)\n",
    "  print(\"[INFO] Training complete.\")\n",
    "  print(\"Best parameters:\", search.best_params_)\n",
    "  print(\"Best CV accuracy:\", search.best_score_)\n",
    "\n",
    "  # Optionally evaluate again with cross_val_score if needed\n",
    "  try:\n",
    "    cv_scores = cross_val_score(\n",
    "      search.best_estimator_,\n",
    "      X_train,\n",
    "      y_train,\n",
    "      cv=5,\n",
    "      scoring='balanced_accuracy'\n",
    "    )\n",
    "    print(\"Generalization accuracy (via cross_val_score):\", cv_scores.mean())\n",
    "  except Exception as e:\n",
    "    print(f\"Cross-validation scoring failed: {e}\")\n",
    "\n",
    "  # Final test prediction\n",
    "  test_predictions = search.best_estimator_.predict(X_test)\n",
    "\n",
    "  return search.best_estimator_, test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26280231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding mapping: ['Adoption' 'Died' 'Euthanasia' 'Return to Owner' 'Transfer']\n",
      "Rare classes:\n",
      "  2: Euthanasia\n",
      "  1: Died\n",
      "Class weights: {0: 0.40387689848121505, 1: 21.35542747358309, 2: 6.445636416352566, 3: 1.3394589383623547, 4: 0.6347361809045227}\n",
      "\n",
      "[INFO] Starting training with 111155 samples\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.6s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   5.9s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   7.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   3.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.7s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   2.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   5.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   5.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   5.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   5.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=5, n_estimators=200; total time=   5.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  10.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  10.5s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   9.3s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   9.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   2.4s\n",
      "[CV] END max_depth=20, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=  10.8s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   4.0s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   3.6s\n",
      "[CV] END max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100; total time=   2.8s\n",
      "[INFO] Training complete.\n",
      "Best parameters: {'n_estimators': 100, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_depth': 10}\n",
      "Best CV accuracy: 0.4822212452399879\n",
      "Generalization accuracy (via cross_val_score): 0.4822212452399879\n",
      "Combined test predictions saved to: ./test_x_rand_trees_predictions_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = df_train.drop(columns=['outcome_type'])\n",
    "y_train = df_train['outcome_type']\n",
    "X_test = df_test\n",
    "\n",
    "# label coder will only be used for the outcome labels\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "print('Encoding mapping:', le.classes_)\n",
    "\n",
    "# Define rare classes that need oversampling \n",
    "rare_classes = [\n",
    "  label for label, count in pd.Series(y_train).value_counts().items()\n",
    "  if count < 0.05 * len(y_train)\n",
    "]\n",
    "print(\"Rare classes:\")\n",
    "for cls in rare_classes:\n",
    "  print(f\"  {cls}: {le.classes_[cls]}\")\n",
    "\n",
    "# Define categorical features - either column names or column indices\n",
    "cat_cols_onehot = ['intake_type', 'intake_condition', 'animal_type', 'sex_upon_intake']\n",
    "cat_cols_freq   = ['breed', 'primary_color']\n",
    "\n",
    "# Frequency encode selected high-cardinality features\n",
    "for col in cat_cols_freq:\n",
    "  # Fit on train, apply same mapping to test\n",
    "  freq_map = X_train[col].value_counts()\n",
    "  X_train[col] = X_train[col].map(freq_map)\n",
    "  # Unseen categories in test get frequency 0\n",
    "  X_test[col]  = X_test[col].map(freq_map).fillna(0)\n",
    "\n",
    "# One-hot encode remaining categorical features\n",
    "X_train = pd.get_dummies(X_train, columns=cat_cols_onehot, drop_first=True)\n",
    "X_test  = pd.get_dummies(X_test,  columns=cat_cols_onehot, drop_first=True)\n",
    "# Align columns: add missing columns in X_test, fill with 0s\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Train model and get test predictions\n",
    "best_model, test_predictions = train_classifier(\n",
    "  X_train=X_train,\n",
    "  y_train=y_train,\n",
    "  X_test=X_test,\n",
    "  rare_classes=rare_classes\n",
    ")\n",
    "\n",
    "# Decode integer predictions back to string labels\n",
    "decoded_preds = le.inverse_transform(test_predictions)\n",
    "\n",
    "# Save predictions using your utility function\n",
    "save_predictions(decoded_preds, model_name='x_rand_trees')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
