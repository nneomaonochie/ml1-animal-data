{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1cfd6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nbformat in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from nbformat) (2.21.1)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from jsonschema>=2.6->nbformat) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from jsonschema>=2.6->nbformat) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from jsonschema>=2.6->nbformat) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from jsonschema>=2.6->nbformat) (0.24.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (4.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from referencing>=0.28.4->jsonschema>=2.6->nbformat) (4.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from imbalanced-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install nbformat\n",
    "%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55270c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Read your notebook (assuming version 4 for example purposes)\n",
    "nb = nbformat.read(\"ml_project.ipynb\", as_version=4)\n",
    "\n",
    "# Normalize the notebook to add missing id fields and other updates\n",
    "nbformat.validator.validate(nb)\n",
    "\n",
    "# Write the normalized notebook back to a file\n",
    "nbformat.write(nb, \"ml_project_normalized.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad82977c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from xgboost) (2.0.2)\n",
      "Requirement already satisfied: scipy in /Users/caseyc/Library/Python/3.9/lib/python/site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "dropped columns\n",
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "cleaned breed\n",
      "cleaned color\n",
      "dropped columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yf/n0zzld6945vfyf6hj9_n1g4c0000gn/T/ipykernel_61979/850107665.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt_series = pd.to_datetime(df['intake_time'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "cleaned breed\n",
      "cleaned color\n",
      "Done running ml_project.ipynb.\n"
     ]
    }
   ],
   "source": [
    "%run ml_project_normalized.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6582ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_classifier(X_train, y_train, X_test, rare_classes, categorical_features):\n",
    "    \"\"\"\n",
    "    Trains an XGBoost model using SMOTE for class balancing and hyperparameter tuning.\n",
    "    CatBoost handles categorical features natively, but here we're demonstrating hyperparameter tuning with XGBoost.\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (pd.DataFrame): Training features.\n",
    "    y_train (pd.Series or np.array): Training target values.\n",
    "    X_test (pd.DataFrame): Test features.\n",
    "    rare_classes (list): List of integer-encoded classes to be oversampled with SMOTE.\n",
    "    categorical_features (list): List of column names for categorical features.\n",
    "    \n",
    "    Returns:\n",
    "        best_estimator: The best estimator from RandomizedSearchCV.\n",
    "        test_predictions: The predicted labels for X_test from the best estimator.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute rare class sampling target\n",
    "    y_series        = pd.Series(y_train)\n",
    "    max_count       = y_series.value_counts().max()\n",
    "    sampling_target = {cls: max_count for cls in rare_classes}\n",
    "\n",
    "    class_labels = np.unique(y_train)\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=class_labels,\n",
    "        y=y_train\n",
    "    )\n",
    "    \n",
    "    # Get indices of categorical features\n",
    "    cat_feature_indices = [X_train.columns.get_loc(col) for col in categorical_features]\n",
    "    \n",
    "    # Define the pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('freq', FunctionTransformer(apply_freq_encode, validate=False)),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True)),\n",
    "        # ('smote', SMOTENC(\n",
    "        #     categorical_features=cat_feature_indices,\n",
    "        #     sampling_strategy=sampling_target,\n",
    "        #     random_state=42\n",
    "        # )),\n",
    "        ('xgb', XGBClassifier(eval_metric='logloss', verbosity=1))\n",
    "    ])\n",
    "    \n",
    "    # Expanded parameter grid for RandomizedSearchCV\n",
    "    param_distributions = {\n",
    "        \"xgb__max_depth\": [2, 3, 5, 7, 9],\n",
    "        \"xgb__learning_rate\": [0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "        \"xgb__n_estimators\": [50, 100, 200, 300, 500],\n",
    "        \"xgb__subsample\": [0.3, 0.5, 0.7, 1.0],\n",
    "        \"xgb__colsample_bytree\": [0.3, 0.5, 0.7, 1.0],\n",
    "        \"xgb__min_child_weight\": [1, 3, 5, 7],\n",
    "        \"xgb__gamma\": [0, 0.1, 0.3, 0.5, 1.0],\n",
    "        \"xgb__reg_alpha\": [0, 0.01, 0.1, 0.5, 1.0],\n",
    "        \"xgb__reg_lambda\": [0.5, 1.0, 1.5, 2.0, 3.0]\n",
    "    }\n",
    "    \n",
    "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=20,\n",
    "        cv=stratified_cv,\n",
    "        scoring='balanced_accuracy',\n",
    "        verbose=3,\n",
    "        random_state=42,\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[INFO] Starting training with {len(X_train)} samples, {len(y_train)} labels\")\n",
    "    \n",
    "    # Fit the model with hyperparameter search\n",
    "    randomized_search.fit(X_train, y_train)\n",
    "    print(f\"[INFO] Training complete. Best model fitted on {len(X_train)} samples.\\n\")\n",
    "    \n",
    "    print('Best parameters:', randomized_search.best_params_)\n",
    "    print('Best cross-validation accuracy:', randomized_search.best_score_)\n",
    "    \n",
    "    # Assuming classification_report_with_accuracy_score is defined elsewhere\n",
    "    cv_scores = cross_val_score(randomized_search.best_estimator_, X_train, y_train, cv=5, verbose=3, scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "    print('Generalization accuracy (via cross_val_score):', cv_scores.mean())\n",
    "    \n",
    "    # Predict on test data\n",
    "    test_predictions = randomized_search.predict(X_test)\n",
    "    \n",
    "    return randomized_search.best_estimator_, test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b302d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "# df_train_downsample = resample(df_train, replace=True, n_samples=10000, random_state=42)\n",
    "# print(df_train_downsample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05976deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding mapping: ['Adoption' 'Died' 'Euthanasia' 'Return to Owner' 'Transfer']\n",
      "Rare classes:\n",
      "  2: Euthanasia\n",
      "  1: Died\n",
      "Training model for Dog data:\n",
      "\n",
      "[INFO] Starting training with 111155 samples, 111155 labels\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "[CV 1/5] END xgb__colsample_bytree=0.3, xgb__gamma=0.3, xgb__learning_rate=0.1, xgb__max_depth=2, xgb__min_child_weight=7, xgb__n_estimators=500, xgb__reg_alpha=0.1, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.369 total time=  10.0s\n",
      "[CV 2/5] END xgb__colsample_bytree=0.3, xgb__gamma=0.3, xgb__learning_rate=0.1, xgb__max_depth=2, xgb__min_child_weight=7, xgb__n_estimators=500, xgb__reg_alpha=0.1, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.374 total time=   9.4s\n",
      "[CV 3/5] END xgb__colsample_bytree=0.3, xgb__gamma=0.3, xgb__learning_rate=0.1, xgb__max_depth=2, xgb__min_child_weight=7, xgb__n_estimators=500, xgb__reg_alpha=0.1, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.370 total time=   9.4s\n",
      "[CV 4/5] END xgb__colsample_bytree=0.3, xgb__gamma=0.3, xgb__learning_rate=0.1, xgb__max_depth=2, xgb__min_child_weight=7, xgb__n_estimators=500, xgb__reg_alpha=0.1, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.364 total time=  10.0s\n",
      "[CV 5/5] END xgb__colsample_bytree=0.3, xgb__gamma=0.3, xgb__learning_rate=0.1, xgb__max_depth=2, xgb__min_child_weight=7, xgb__n_estimators=500, xgb__reg_alpha=0.1, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.371 total time=   9.5s\n",
      "[CV 1/5] END xgb__colsample_bytree=0.7, xgb__gamma=0.5, xgb__learning_rate=0.1, xgb__max_depth=2, xgb__min_child_weight=5, xgb__n_estimators=100, xgb__reg_alpha=0.1, xgb__reg_lambda=2.0, xgb__subsample=1.0;, score=0.355 total time=   2.7s\n",
      "[CV 2/5] END xgb__colsample_bytree=0.7, xgb__gamma=0.5, xgb__learning_rate=0.1, xgb__max_depth=2, xgb__min_child_weight=5, xgb__n_estimators=100, xgb__reg_alpha=0.1, xgb__reg_lambda=2.0, xgb__subsample=1.0;, score=0.350 total time=   2.7s\n",
      "[CV 3/5] END xgb__colsample_bytree=0.7, xgb__gamma=0.5, xgb__learning_rate=0.1, xgb__max_depth=2, xgb__min_child_weight=5, xgb__n_estimators=100, xgb__reg_alpha=0.1, xgb__reg_lambda=2.0, xgb__subsample=1.0;, score=0.352 total time=   2.7s\n",
      "[CV 4/5] END xgb__colsample_bytree=0.7, xgb__gamma=0.5, xgb__learning_rate=0.1, xgb__max_depth=2, xgb__min_child_weight=5, xgb__n_estimators=100, xgb__reg_alpha=0.1, xgb__reg_lambda=2.0, xgb__subsample=1.0;, score=0.348 total time=   2.7s\n",
      "[CV 5/5] END xgb__colsample_bytree=0.7, xgb__gamma=0.5, xgb__learning_rate=0.1, xgb__max_depth=2, xgb__min_child_weight=5, xgb__n_estimators=100, xgb__reg_alpha=0.1, xgb__reg_lambda=2.0, xgb__subsample=1.0;, score=0.353 total time=   2.8s\n",
      "[CV 1/5] END xgb__colsample_bytree=0.3, xgb__gamma=0.3, xgb__learning_rate=0.2, xgb__max_depth=2, xgb__min_child_weight=7, xgb__n_estimators=500, xgb__reg_alpha=0.01, xgb__reg_lambda=2.0, xgb__subsample=0.3;, score=0.375 total time=   9.5s\n",
      "[CV 2/5] END xgb__colsample_bytree=0.3, xgb__gamma=0.3, xgb__learning_rate=0.2, xgb__max_depth=2, xgb__min_child_weight=7, xgb__n_estimators=500, xgb__reg_alpha=0.01, xgb__reg_lambda=2.0, xgb__subsample=0.3;, score=0.377 total time=   9.6s\n",
      "[CV 3/5] END xgb__colsample_bytree=0.3, xgb__gamma=0.3, xgb__learning_rate=0.2, xgb__max_depth=2, xgb__min_child_weight=7, xgb__n_estimators=500, xgb__reg_alpha=0.01, xgb__reg_lambda=2.0, xgb__subsample=0.3;, score=0.378 total time=   9.7s\n",
      "[CV 4/5] END xgb__colsample_bytree=0.3, xgb__gamma=0.3, xgb__learning_rate=0.2, xgb__max_depth=2, xgb__min_child_weight=7, xgb__n_estimators=500, xgb__reg_alpha=0.01, xgb__reg_lambda=2.0, xgb__subsample=0.3;, score=0.370 total time=   9.5s\n",
      "[CV 5/5] END xgb__colsample_bytree=0.3, xgb__gamma=0.3, xgb__learning_rate=0.2, xgb__max_depth=2, xgb__min_child_weight=7, xgb__n_estimators=500, xgb__reg_alpha=0.01, xgb__reg_lambda=2.0, xgb__subsample=0.3;, score=0.376 total time=   9.4s\n",
      "[CV 1/5] END xgb__colsample_bytree=0.5, xgb__gamma=0.3, xgb__learning_rate=0.05, xgb__max_depth=5, xgb__min_child_weight=7, xgb__n_estimators=300, xgb__reg_alpha=0.01, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.385 total time=  11.0s\n",
      "[CV 2/5] END xgb__colsample_bytree=0.5, xgb__gamma=0.3, xgb__learning_rate=0.05, xgb__max_depth=5, xgb__min_child_weight=7, xgb__n_estimators=300, xgb__reg_alpha=0.01, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.380 total time=  10.9s\n",
      "[CV 3/5] END xgb__colsample_bytree=0.5, xgb__gamma=0.3, xgb__learning_rate=0.05, xgb__max_depth=5, xgb__min_child_weight=7, xgb__n_estimators=300, xgb__reg_alpha=0.01, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.378 total time=  10.9s\n",
      "[CV 4/5] END xgb__colsample_bytree=0.5, xgb__gamma=0.3, xgb__learning_rate=0.05, xgb__max_depth=5, xgb__min_child_weight=7, xgb__n_estimators=300, xgb__reg_alpha=0.01, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.375 total time=  11.5s\n",
      "[CV 5/5] END xgb__colsample_bytree=0.5, xgb__gamma=0.3, xgb__learning_rate=0.05, xgb__max_depth=5, xgb__min_child_weight=7, xgb__n_estimators=300, xgb__reg_alpha=0.01, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.379 total time=  11.1s\n",
      "[CV 1/5] END xgb__colsample_bytree=0.5, xgb__gamma=0, xgb__learning_rate=0.01, xgb__max_depth=9, xgb__min_child_weight=5, xgb__n_estimators=100, xgb__reg_alpha=0.5, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.357 total time=   8.5s\n",
      "[CV 2/5] END xgb__colsample_bytree=0.5, xgb__gamma=0, xgb__learning_rate=0.01, xgb__max_depth=9, xgb__min_child_weight=5, xgb__n_estimators=100, xgb__reg_alpha=0.5, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.349 total time=   8.5s\n",
      "[CV 3/5] END xgb__colsample_bytree=0.5, xgb__gamma=0, xgb__learning_rate=0.01, xgb__max_depth=9, xgb__min_child_weight=5, xgb__n_estimators=100, xgb__reg_alpha=0.5, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.350 total time=   8.8s\n",
      "[CV 4/5] END xgb__colsample_bytree=0.5, xgb__gamma=0, xgb__learning_rate=0.01, xgb__max_depth=9, xgb__min_child_weight=5, xgb__n_estimators=100, xgb__reg_alpha=0.5, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.355 total time=   8.7s\n",
      "[CV 5/5] END xgb__colsample_bytree=0.5, xgb__gamma=0, xgb__learning_rate=0.01, xgb__max_depth=9, xgb__min_child_weight=5, xgb__n_estimators=100, xgb__reg_alpha=0.5, xgb__reg_lambda=3.0, xgb__subsample=0.7;, score=0.352 total time=   8.9s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# df_train = resample(df_train, replace=True, n_samples=25000, random_state=42)\n",
    "\n",
    "## Encode targets with LabelEncoder\n",
    "# Dog encoding\n",
    "le = LabelEncoder()\n",
    "X_train = df_train.drop(columns=['outcome_type'])\n",
    "\n",
    "y_train = df_train['outcome_type']\n",
    "y_train = le.fit_transform(y_train)\n",
    "print('Encoding mapping:', le.classes_)\n",
    "\n",
    "# Define rare classes that need oversampling \n",
    "rare_classes = [\n",
    "  label for label, count in pd.Series(y_train).value_counts().items()\n",
    "  if count < 0.05 * len(y_train)\n",
    "]\n",
    "print(\"Rare classes:\")\n",
    "for cls in rare_classes:\n",
    "  print(f\"  {cls}: {le.classes_[cls]}\")\n",
    "\n",
    "cat_cols = {'intake_type', 'intake_condition', 'animal_type', 'sex_upon_intake', 'breed', 'primary_color'}\n",
    "categorical_features = [col for col in X_train.columns if col in cat_cols]\n",
    "\n",
    "print(\"Training model for Dog data:\")\n",
    "best_model, test_predictions = train_classifier(\n",
    "  X_train=X_train,\n",
    "  y_train=y_train,\n",
    "  X_test=df_test,\n",
    "  rare_classes=rare_classes,\n",
    "  categorical_features=categorical_features\n",
    ")\n",
    "predictions = le.inverse_transform(test_predictions)\n",
    "\n",
    "save_predictions(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a6d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined test predictions saved to: ./test_xg_boost_predictions_combined.csv\n"
     ]
    }
   ],
   "source": [
    "predictions = le.inverse_transform(test_predictions)\n",
    "\n",
    "save_predictions(predictions, 'xg_boost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa59f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
