{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c57ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: xgboost in /u/nneoma/.local/lib/python3.8/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /u/nneoma/.local/lib/python3.8/site-packages (from xgboost) (1.24.4)\n",
      "Requirement already satisfied: scipy in /u/nneoma/.local/lib/python3.8/site-packages (from xgboost) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "dropped columns\n",
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "cleaned color\n",
      "cleaned breed\n",
      "dropped columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_898512/1199118304.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt_series = pd.to_datetime(df['intake_time'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "cleaned color\n",
      "cleaned breed\n",
      "Done running ml_project.ipynb.\n"
     ]
    }
   ],
   "source": [
    "import nbformat\n",
    "\n",
    "# Read your notebook (assuming version 4 for example purposes)\n",
    "nb = nbformat.read(\"ml_project.ipynb\", as_version=4)\n",
    "\n",
    "# Normalize the notebook to add missing id fields and other updates\n",
    "nbformat.validator.validate(nb)\n",
    "\n",
    "# Write the normalized notebook back to a file\n",
    "nbformat.write(nb, \"ml_project_normalized.ipynb\")\n",
    "\n",
    "%run ml_project_normalized.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f860b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# # Apply PCA to retain 95% of variance\n",
    "# pca = PCA(n_components=0.95)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# X_test  = pca.transform(X_test)\n",
    "\n",
    "# print(f\"[INFO] Reduced dimensions with PCA: {X_train.shape[1]} components retained.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10e5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_RF_classifier(X_train, y_train, X_test, cat_cols, target_samples_per_class=3000):\n",
    "  \"\"\"\n",
    "  Trains an Extremely Randomized Trees model using class weighting and RandomizedSearchCV.\n",
    "\n",
    "  Parameters:\n",
    "    X_train (pd.DataFrame or np.ndarray): Fully preprocessed training features.\n",
    "    y_train (array-like): Encoded training labels.\n",
    "    X_test (pd.DataFrame or np.ndarray): Fully preprocessed test features.\n",
    "    rare_classes (list): List of int-encoded rare class labels.\n",
    "    target_samples_per_class (int): Target number of samples per rare class (currently unused).\n",
    "\n",
    "  Returns:\n",
    "    best_estimator: Trained model.\n",
    "    test_predictions: Predictions on the test set.\n",
    "  \"\"\"\n",
    "\n",
    "  '''\n",
    "    # Make copies to avoid modifying original data\n",
    "  X_train = X_train.copy()\n",
    "  X_test = X_test.copy()\n",
    "\n",
    "  # Drop hour_sin and hour_cos if they exist\n",
    "  drop_cols = ['hour_sin', 'hour_cos']\n",
    "  existing_cols = [col for col in drop_cols if col in X_train.columns]\n",
    "  '''\n",
    "  all_cols = X_train.columns.tolist()\n",
    "\n",
    "  # Convert column names to positional indices\n",
    "  cat_indices = [all_cols.index(col) for col in cat_cols]\n",
    "\n",
    "\n",
    "\n",
    "  # Compute class weights for imbalanced training set\n",
    "  class_labels = np.unique(y_train)\n",
    "  class_weights = compute_class_weight('balanced', classes=class_labels, y=y_train)\n",
    "  class_weight_dict = dict(zip(class_labels, class_weights))\n",
    "  print(\"Class weights:\", class_weight_dict)\n",
    "\n",
    "  categorical_transformer = Pipeline(steps=[\n",
    "    #   ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values (if any)\n",
    "      ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-Hot Encode categorical features\n",
    "  ])\n",
    "\n",
    "    # Combine both transformations into a single ColumnTransformer\n",
    "  preprocessor = ColumnTransformer(\n",
    "      transformers=[\n",
    "          ('cat', categorical_transformer, cat_indices)\n",
    "      ])\n",
    "\n",
    "  # Create a pipeline that first transforms the data and then applies Logistic Regression\n",
    "  pipeline = Pipeline(steps=[\n",
    "    #  ('smote', SMOTENC(categorical_features=cat_cols, random_state=42, sampling_strategy=sampling_strategy)),\n",
    "      ('preprocessor', preprocessor),\n",
    "      ('rf', RandomForestClassifier(class_weight=class_weight_dict, random_state=42, n_jobs=-1))  # You can adjust max_iter as needed\n",
    "  ])\n",
    "\n",
    "\n",
    "  '''\n",
    "  # Build model pipeline (only model step, encoding was done externally)\n",
    "  model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    class_weight=class_weight_dict,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    "  )\n",
    "  '''\n",
    "  # Define hyperparameter search space\n",
    "  param_dist = {\n",
    "    'rf__n_estimators': [100, 200, 300],\n",
    "    'rf__max_depth': [10, 20, None], \n",
    "    'rf__min_samples_split': [2, 5, 10],\n",
    "    'rf__min_samples_leaf': [1, 2, 4]\n",
    "  }\n",
    "  \n",
    "\n",
    "  search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=5,\n",
    "    cv=skf,\n",
    "    scoring='balanced_accuracy',\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=2\n",
    "  )\n",
    "\n",
    "  print(f\"\\n[INFO] Starting training with {len(X_train)} samples\")\n",
    "  search.fit(X_train, y_train)\n",
    "  print(\"[INFO] Training complete.\")\n",
    "\n",
    "  # Plotting feature importance\n",
    "  import matplotlib.pyplot as plt\n",
    "  \n",
    "  '''\n",
    "  feature_names = X_train.columns  # assumes X_train is still a DataFrame\n",
    "  importances   = search.best_estimator_.feature_importances_\n",
    "  indices       = np.argsort(importances)[::-1]\n",
    "\n",
    "  print(\"\\\\n[INFO] Feature ranking:\")\n",
    "  for rank, idx in enumerate(indices[:20], 1):\n",
    "    print(f\"{rank}. {feature_names[idx]} â€” importance: {importances[idx]:.4f}\")\n",
    "\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  plt.title(\"Top 20 Feature Importances\")\n",
    "  plt.bar(range(20), importances[indices[:20]], align=\"center\")\n",
    "  plt.xticks(range(20), [feature_names[i] for i in indices[:20]], rotation=45, ha='right')\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  '''\n",
    "\n",
    "  print(\"Best parameters:\", search.best_params_)\n",
    "  print(\"Best CV accuracy:\", search.best_score_)\n",
    "\n",
    "  # Optionally evaluate again with cross_val_score if needed\n",
    "  try:\n",
    "    cv_scores = cross_val_score(\n",
    "      search.best_estimator_,\n",
    "      X_train,\n",
    "      y_train,\n",
    "      cv=skf,\n",
    "      scoring='balanced_accuracy',\n",
    "      verbose=3\n",
    "    )\n",
    "    print(\"Generalization accuracy (via cross_val_score):\", cv_scores.mean())\n",
    "  except Exception as e:\n",
    "    print(f\"Cross-validation scoring failed: {e}\")\n",
    "\n",
    "  # Final test prediction\n",
    "  test_predictions = search.best_estimator_.predict(X_test)\n",
    "\n",
    "  return search.best_estimator_, test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26280231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding mapping: ['Adoption' 'Died' 'Euthanasia' 'Return to Owner' 'Transfer']\n",
      "Rare classes:\n",
      "  2: Euthanasia\n",
      "  1: Died\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cat_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m X_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mreindex(columns\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mcolumns, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Train model and get test predictions\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m best_model, test_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_RF_classifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m  \u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m  \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m  \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m  \u001b[49m\u001b[43mrare_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrare_classes\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Decode integer predictions back to string labels\u001b[39;00m\n\u001b[1;32m     53\u001b[0m decoded_preds \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39minverse_transform(test_predictions)\n",
      "Cell \u001b[0;32mIn[12], line 37\u001b[0m, in \u001b[0;36mtrain_RF_classifier\u001b[0;34m(X_train, y_train, X_test, rare_classes, target_samples_per_class)\u001b[0m\n\u001b[1;32m     34\u001b[0m all_cols \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Convert column names to positional indices\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m cat_indices \u001b[38;5;241m=\u001b[39m [all_cols\u001b[38;5;241m.\u001b[39mindex(col) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[43mcat_cols\u001b[49m]\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Compute class weights for imbalanced training set\u001b[39;00m\n\u001b[1;32m     42\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_train)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cat_cols' is not defined"
     ]
    }
   ],
   "source": [
    "# Encode the target variable.\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_test = le.fit_transform(y_test)\n",
    "print('Encoding mapping:', le.classes_)\n",
    "\n",
    "best_model, test_predictions = train_Bclassifier(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    cat_cols=cat_cols,\n",
    "    num_cols=num_cols\n",
    "\n",
    ")\n",
    "\n",
    "# Convert predictions back to original labels.\n",
    "classification_report_with_accuracy_score(y_test, test_predictions)\n",
    "#print(importance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
