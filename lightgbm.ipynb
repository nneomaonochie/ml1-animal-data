{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cfd6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nbformat\n",
    "%pip install imbalanced-learn\n",
    "%pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55270c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "nb = nbformat.read(\"ml_project.ipynb\", as_version=4)\n",
    "\n",
    "# Normalize the notebook to add missing id fields and other updates\n",
    "nbformat.validator.validate(nb)\n",
    "\n",
    "# Write the normalized notebook back to a file\n",
    "nbformat.write(nb, \"ml_project_normalized.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ml_project_normalized.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.float = float  # Patch for libraries using deprecated np.float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6582ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def train_classifier_lightgbm(\n",
    "    X_train, y_train, X_test, rare_classes, cat_cols, custom_thresholds=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a LightGBM model using hyperparameter tuning.\n",
    "    Applies custom thresholding during prediction if specified and compares standard vs. custom accuracy.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        y_train (pd.Series or np.array): Training target values.\n",
    "        X_test (pd.DataFrame): Test features.\n",
    "        rare_classes (list): List of integer-encoded classes to be oversampled.\n",
    "        cat_cols (set or list): Categorical columns to one-hot encode.\n",
    "        custom_thresholds (dict, optional): Dict of class_label -> threshold.\n",
    "            e.g., {0: 0.7, 1: 0.5, 2: 0.5} for class 0 = supermajority.\n",
    "\n",
    "    Returns:\n",
    "        best_estimator: Trained pipeline.\n",
    "        test_predictions: Custom-thresholded predictions.\n",
    "        df_importances: Feature importances from LightGBM.\n",
    "    \"\"\"\n",
    "    # Compute balanced sample weights\n",
    "    class_labels = np.unique(y_train)\n",
    "    class_weights = compute_class_weight('balanced', classes=class_labels, y=y_train)\n",
    "    class_weight_dict = dict(zip(class_labels, class_weights))\n",
    "    sample_weights = np.array([class_weight_dict[y] for y in y_train])\n",
    "\n",
    "    # Ensure categorical features are strings\n",
    "    categorical_features = [col for col in X_train.columns if col in cat_cols]\n",
    "    for col in categorical_features:\n",
    "        X_train[col] = X_train[col].astype(str)\n",
    "        X_test[col] = X_test[col].astype(str)\n",
    "\n",
    "    # Define preprocessing pipeline\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    # Build pipeline with LightGBM classifier\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"lgbm\", LGBMClassifier(objective='multiclass', verbosity=-1))\n",
    "    ])\n",
    "\n",
    "    # Hyperparameter search space for LGBM\n",
    "    param_distributions = {\n",
    "        \"lgbm__num_leaves\": randint(20, 150),\n",
    "        \"lgbm__learning_rate\": uniform(0.01, 0.5),\n",
    "        \"lgbm__n_estimators\": randint(50, 500),\n",
    "    \"lgbm__subsample\":        uniform(0.3, 0.7),\n",
    "    \"lgbm__colsample_bytree\": uniform(0.3, 0.7),\n",
    "        \"lgbm__min_child_samples\": randint(1, 50),\n",
    "        \"lgbm__min_split_gain\": uniform(0, 1.0),\n",
    "        \"lgbm__reg_alpha\": uniform(0, 1.0),\n",
    "        \"lgbm__reg_lambda\": uniform(0.5, 2.5)\n",
    "    }\n",
    "\n",
    "    # Stratified cross-validation setup\n",
    "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Randomized search for best hyperparameters\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=5,\n",
    "        cv=stratified_cv,\n",
    "        scoring='balanced_accuracy',\n",
    "        verbose=3,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[INFO] Starting training with {len(X_train)} samples and {len(y_train)} labels\")\n",
    "    randomized_search.fit(X_train, y_train, **{'lgbm__sample_weight': sample_weights})\n",
    "    print(f\"[INFO] Training complete. Best model fitted on {len(X_train)} samples.\\n\")\n",
    "\n",
    "    # Debug one hot encoded features by grabbing column names during training\n",
    "    best_preprocessor = randomized_search.best_estimator_.named_steps['preprocessor']\n",
    "    encoded_X_train = best_preprocessor.transform(X_train)\n",
    "    if hasattr(encoded_X_train, \"toarray\"):\n",
    "        encoded_X_train = encoded_X_train.toarray()\n",
    "    feature_names = best_preprocessor.get_feature_names_out()\n",
    "    print(\"Sample of transformed training features:\")\n",
    "    print(pd.DataFrame(encoded_X_train, columns=feature_names).head())\n",
    "\n",
    "    # Print best parameters and CV score\n",
    "    print('Best parameters:', randomized_search.best_params_)\n",
    "    print('Best cross-validation balanced accuracy:', randomized_search.best_score_)\n",
    "\n",
    "    # Generalization accuracy via cross_val_score\n",
    "    cv_scores = cross_val_score(\n",
    "        randomized_search.best_estimator_,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        cv=5,\n",
    "        verbose=3,\n",
    "        scoring='balanced_accuracy'\n",
    "    )\n",
    "    print('Generalization balanced accuracy:', cv_scores.mean())\n",
    "\n",
    "    # Prediction with optional custom thresholds\n",
    "    lgbm_model = randomized_search.best_estimator_.named_steps['lgbm']\n",
    "    class_names = lgbm_model.classes_\n",
    "\n",
    "\n",
    "    # Further helps with class imbalance by requiring certain labels achieve a higher/lower prediciton confidence\n",
    "    def apply_custom_thresholds(probabilities, class_names, thresholds_dict):\n",
    "        thresholds = np.array([thresholds_dict.get(cls, 0.5) for cls in class_names])\n",
    "        preds = []\n",
    "        for row in probabilities:\n",
    "            passed = row >= thresholds\n",
    "            if not passed.any():\n",
    "                pred = class_names[np.argmax(row)]\n",
    "            else:\n",
    "                pred = class_names[np.argmax(passed * row)]\n",
    "            preds.append(pred)\n",
    "        return np.array(preds)\n",
    "\n",
    "    # get probability distribution for all test records for all labels for custom thresholding\n",
    "    proba_test = randomized_search.predict_proba(X_test)\n",
    "    if custom_thresholds:\n",
    "        print(\"[INFO] Applying custom threshold logic to test set\")\n",
    "        test_predictions = apply_custom_thresholds(proba_test, class_names, custom_thresholds)\n",
    "    else:\n",
    "        test_predictions = randomized_search.predict(X_test)\n",
    "\n",
    "    # Compare accuracies on training set if custom thresholds provided\n",
    "    if custom_thresholds:\n",
    "        proba_train = randomized_search.predict_proba(X_train)\n",
    "        preds_custom_train = apply_custom_thresholds(proba_train, class_names, custom_thresholds)\n",
    "        acc_standard = accuracy_score(y_train, randomized_search.predict(X_train))\n",
    "        acc_custom = accuracy_score(y_train, preds_custom_train)\n",
    "        bal_acc_standard = balanced_accuracy_score(y_train, randomized_search.predict(X_train))\n",
    "        bal_acc_custom = balanced_accuracy_score(y_train, preds_custom_train)\n",
    "\n",
    "        print(f\"\\nAccuracy Comparison on Training Set:\")\n",
    "        print(f\"Standard Accuracy: {acc_standard:.4f}\")\n",
    "        print(f\"Custom Threshold Accuracy: {acc_custom:.4f}\")\n",
    "        print(f\"Standard Balanced Accuracy: {bal_acc_standard:.4f}\")\n",
    "        print(f\"Custom Threshold Balanced Accuracy: {bal_acc_custom:.4f}\")\n",
    "\n",
    "    # Extract feature importances\n",
    "    importances = lgbm_model.feature_importances_\n",
    "    df_importances = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    })\n",
    "\n",
    "    return randomized_search.best_estimator_, test_predictions, df_importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc94e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near deadline attempt to add location into training. Little attempt before to try feature engineering on location.\n",
    "def add_in_austin(df, df_loc):\n",
    "    \"\"\"\n",
    "    Adds a binary column 'in_austin' that is 1 if 'austin' appears\n",
    "    (caseâ€‘insensitive) in the 'location' field, else 0.\n",
    "    \"\"\"\n",
    "    df['in_austin'] = (\n",
    "        df_loc\n",
    "        .str.contains('austin', case=False, na=False)\n",
    "        .astype(int)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b302d895",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = df_train.copy()\n",
    "# X_train = add_in_austin(X_train, df_train_location) # df_train_location used to be populated in ml_project\n",
    "X_train = bucket_seasons(X_train)\n",
    "X_train = bucket_days(X_train)\n",
    "X_train = X_train.drop(columns=['intake_month', 'intake_hour'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b7ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test = df_test.copy()\n",
    "# X_test = add_in_austin(X_test, df_test_location)\n",
    "X_test = bucket_seasons(X_test)\n",
    "X_test = bucket_days(X_test)\n",
    "X_test = X_test.drop(columns=['intake_month', 'intake_hour'])\n",
    "X_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05976deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(\"ignore\", category=DataConversionWarning)\n",
    "\n",
    "# Encode the target variable.\n",
    "le = LabelEncoder()\n",
    "y_train = X_train['outcome_type']\n",
    "X_train = X_train.drop(columns=['outcome_type'])\n",
    "y_train = le.fit_transform(y_train)\n",
    "print('Encoding mapping:', le.classes_)\n",
    "\n",
    "# Identify rare classes that need oversampling.\n",
    "rare_classes = [\n",
    "    label for label, count in pd.Series(y_train).value_counts().items()\n",
    "    if count < 0.05 * len(y_train)\n",
    "]\n",
    "print(\"Rare classes:\")\n",
    "for cls in rare_classes:\n",
    "    print(f\"  {cls}: {le.classes_[cls]}\")\n",
    "\n",
    "# Define column groups (this example will one-hot encode all columns, so cat_cols and num_cols are not used in the transformer).\n",
    "cat_cols = {'intake_type', 'intake_condition', 'animal_type', 'sex_upon_intake', 'breed', 'intake_month', 'intake_hour', 'intake_year', 'season', 'time_of_day', 'size', 'primary_color'}\n",
    "categorical_features = [col for col in X_train.columns if col in cat_cols]\n",
    "\n",
    "'''\n",
    "# cat_cols_freq = ['primary_color']\n",
    "\n",
    "\n",
    "# Frequency encode selected high-cardinality features, old thing we tried before we realized this is data leakage\n",
    "# for col in cat_cols_freq:\n",
    "#   freq_map = X_train[col].value_counts()\n",
    "#   X_train[col] = X_train[col].map(freq_map)\n",
    "#   X_test[col]  = X_test[col].map(freq_map).fillna(0)\n",
    "'''\n",
    "# Train the classifier with the refactored pipeline.\n",
    "\n",
    "custom_thresholds = {\n",
    "    0: 0.6, # Adopted\n",
    "    1: 0.5, # Died\n",
    "    2: 0.5, # Euthanasia\n",
    "    3: 0.5, # Return to Owner\n",
    "    4: 0.5  # Transfer\n",
    "}\n",
    "\n",
    "best_model, test_predictions, df_importances = train_classifier_lightgbm(\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    X_test=X_test,\n",
    "    rare_classes=rare_classes,\n",
    "    cat_cols=categorical_features,\n",
    "    custom_thresholds=custom_thresholds\n",
    ")\n",
    "\n",
    "# Convert predictions back to original labels.\n",
    "predictions = le.inverse_transform(test_predictions)\n",
    "\n",
    "# Save predictions; assumes save_predictions is defined elsewhere.\n",
    "classification_report_with_accuracy_score(y_test, predictions)\n",
    "#save_predictions(predictions, 'lightgbm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e70d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plots feature importance values to see how our feature engineering is performing and what features we could possibly leave out\n",
    "df_plot = df_importances.sort_values(by='importance', ascending=True)\n",
    "\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.barh(df_plot['feature'], df_plot['importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Feature Importances from light gbm')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
