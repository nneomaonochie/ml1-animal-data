{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71ce74e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d87c920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "# Read your notebook (assuming version 4 for example purposes)\n",
    "nb = nbformat.read(\"ml_project.ipynb\", as_version=4)\n",
    "\n",
    "# Normalize the notebook to add missing id fields and other updates\n",
    "nbformat.validator.validate(nb)\n",
    "\n",
    "# Write the normalized notebook back to a file\n",
    "nbformat.write(nb, \"ml_project_normalized.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b38f54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in /u/nneoma/.local/lib/python3.8/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from xgboost) (1.17.4)\n",
      "Requirement already satisfied: scipy in /u/nneoma/.local/lib/python3.8/site-packages (from xgboost) (1.10.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "dropped columns\n",
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "cleaned breed\n",
      "cleaned color\n",
      "dropped columns\n",
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "cleaned breed\n",
      "cleaned color\n",
      "Done running ml_project.ipynb.\n"
     ]
    }
   ],
   "source": [
    "%run ml_project_normalized.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3b5d9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0db76efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def train_SVM_classifier(X_train, y_train, X_test):\n",
    "\n",
    "    class_weights = get_class_weights(y_train)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('smote', SMOTENC(categorical_features=[0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11], random_state=42)),# 'intake_type', 'intake_condition', 'animal_type', 'sex_upon_intake', 'breed', 'is_mix', 'primary_color'], random_state=42)),\n",
    "        ('freq', FunctionTransformer(apply_freq_encode, validate=False)),\n",
    "        ('onehot', OneHotEncoder( handle_unknown='ignore', sparse_output=True)),\n",
    "        ('linsvc', LinearSVC(class_weight=class_weights, max_iter=10000, dual=False, verbose=2))\n",
    "    ])\n",
    "\n",
    "    # Set up parameter distributions for XGBoost.\n",
    "    param_distributions = {\n",
    "    'linsvc__C': [0.01, 0.1, 1, 2, 4, 8, 10],\n",
    "    'linsvc__loss': ['squared_hinge', 'hinge'],  # 'hinge' is okay too, but 'squared_hinge' usually works better\n",
    "   # 'linsvc__penalty': ['l2'],  # 'l1' can only be used with dual=False and 'liblinear' solver\n",
    "    'linsvc__max_iter': [1000, 5000, 10000],\n",
    "    'linsvc__multi_class': ['ovr', 'crammer_singer'],  # try both strategies\n",
    "    }\n",
    "    # Perform hyperparameter search using RandomizedSearchCV.\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        estimator=pipeline, \n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=1,\n",
    "        cv=skf, \n",
    "        scoring=balanced_acc_scorer, \n",
    "        verbose=3,\n",
    "    )\n",
    "\n",
    "\n",
    "        \n",
    "    randomized_search.fit(X_train, y_train)\n",
    "    \n",
    "    print('Best parameters:', randomized_search.best_params_)\n",
    "    print('Best cross-validation accuracy:', randomized_search.best_score_)\n",
    "    \n",
    "    cv_scores = cross_val_score(randomized_search.best_estimator_, X_train, y_train, cv=skf, verbose=3, scoring=balanced_acc_scorer)\n",
    "    print('Generalization Balanced accuracy (via cross_val_score):', cv_scores.mean())\n",
    "\n",
    "    # Make predictions on the test set using the best estimator.\n",
    "    test_predictions = randomized_search.predict(X_test)\n",
    "        \n",
    "    return randomized_search.best_estimator_, test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffd8f78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[CV 1/5] END linsvc__C=0.1, linsvc__loss=hinge, linsvc__max_iter=1000, linsvc__multi_class=ovr;, score=nan total time=   0.1s\n",
      "[CV 2/5] END linsvc__C=0.1, linsvc__loss=hinge, linsvc__max_iter=1000, linsvc__multi_class=ovr;, score=nan total time=   0.1s\n",
      "[CV 3/5] END linsvc__C=0.1, linsvc__loss=hinge, linsvc__max_iter=1000, linsvc__multi_class=ovr;, score=nan total time=   0.1s\n",
      "[CV 4/5] END linsvc__C=0.1, linsvc__loss=hinge, linsvc__max_iter=1000, linsvc__multi_class=ovr;, score=nan total time=   0.1s\n",
      "[CV 5/5] END linsvc__C=0.1, linsvc__loss=hinge, linsvc__max_iter=1000, linsvc__multi_class=ovr;, score=nan total time=   0.1s\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py\", line 413, in _get_column_indices\n    idx = _safe_indexing(np.arange(n_columns), key)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py\", line 355, in _safe_indexing\n    return _array_indexing(X, indices, indices_dtype, axis=axis)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py\", line 184, in _array_indexing\n    return array[key] if axis == 0 else array[:, key]\nIndexError: index 9 is out of bounds for axis 0 with size 9\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/pipeline.py\", line 329, in fit\n    Xt, yt = self._fit(X, y, routed_params)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/pipeline.py\", line 265, in _fit\n    X, y, fitted_transformer = fit_resample_one_cached(\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/pipeline.py\", line 1057, in _fit_resample_one\n    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/base.py\", line 208, in fit_resample\n    return super().fit_resample(X, y)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/base.py\", line 112, in fit_resample\n    output = self._fit_resample(X, y)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/over_sampling/_smote/base.py\", line 653, in _fit_resample\n    self._validate_column_types(X)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/over_sampling/_smote/base.py\", line 623, in _validate_column_types\n    _get_column_indices(X, self.categorical_features)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py\", line 415, in _get_column_indices\n    raise ValueError(\nValueError: all features must be in [0, 8] or [-9, 0]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 36\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Perform hyperparameter search using RandomizedSearchCV.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m randomized_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[1;32m     26\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipeline, \n\u001b[1;32m     27\u001b[0m     param_distributions\u001b[38;5;241m=\u001b[39mparam_distributions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     32\u001b[0m )\n\u001b[0;32m---> 36\u001b[0m \u001b[43mrandomized_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m'\u001b[39m, randomized_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest cross-validation accuracy:\u001b[39m\u001b[38;5;124m'\u001b[39m, randomized_search\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:1809\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1808\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1809\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1811\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[1;32m   1812\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1813\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m!=\u001b[39m n_candidates \u001b[38;5;241m*\u001b[39m n_splits:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv.split and cv.get_n_splits returned \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minconsistent results. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplits, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_splits, \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m n_candidates)\n\u001b[1;32m    873\u001b[0m     )\n\u001b[0;32m--> 875\u001b[0m \u001b[43m_warn_or_raise_about_fit_failures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;66;03m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;66;03m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;66;03m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoring):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:414\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_failed_fits \u001b[38;5;241m==\u001b[39m num_fits:\n\u001b[1;32m    408\u001b[0m     all_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAll the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt is very likely that your model is misconfigured.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou can try to debug the error by setting error_score=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    412\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    413\u001b[0m     )\n\u001b[0;32m--> 414\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     some_fits_failed_message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mnum_failed_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m fits failed out of a total of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_fits\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    419\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe score on these train-test partitions for these parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBelow are more details about the failures:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfit_errors_summary\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: \nAll the 5 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n5 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py\", line 413, in _get_column_indices\n    idx = _safe_indexing(np.arange(n_columns), key)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py\", line 355, in _safe_indexing\n    return _array_indexing(X, indices, indices_dtype, axis=axis)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py\", line 184, in _array_indexing\n    return array[key] if axis == 0 else array[:, key]\nIndexError: index 9 is out of bounds for axis 0 with size 9\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/base.py\", line 1152, in wrapper\n    return fit_method(estimator, *args, **kwargs)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/pipeline.py\", line 329, in fit\n    Xt, yt = self._fit(X, y, routed_params)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/pipeline.py\", line 265, in _fit\n    X, y, fitted_transformer = fit_resample_one_cached(\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/pipeline.py\", line 1057, in _fit_resample_one\n    X_res, y_res = sampler.fit_resample(X, y, **params.get(\"fit_resample\", {}))\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/base.py\", line 208, in fit_resample\n    return super().fit_resample(X, y)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/base.py\", line 112, in fit_resample\n    output = self._fit_resample(X, y)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/over_sampling/_smote/base.py\", line 653, in _fit_resample\n    self._validate_column_types(X)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/imblearn/over_sampling/_smote/base.py\", line 623, in _validate_column_types\n    _get_column_indices(X, self.categorical_features)\n  File \"/u/nneoma/.local/lib/python3.8/site-packages/sklearn/utils/__init__.py\", line 415, in _get_column_indices\n    raise ValueError(\nValueError: all features must be in [0, 8] or [-9, 0]\n"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "X = df_train.drop(columns=['outcome_type'])\n",
    "y = df_train['outcome_type']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.20, random_state=42)\n",
    "\n",
    "\n",
    "class_weights_cv = get_class_weights(y_train)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTENC(categorical_features=[0, 1, 2, 3, 5, 6, 7, 8, 9, 10, 11], random_state=42)),# 'intake_type', 'intake_condition', 'animal_type', 'sex_upon_intake', 'breed', 'is_mix', 'primary_color'], random_state=42)),\n",
    "    ('freq', FunctionTransformer(apply_freq_encode, validate=False)),\n",
    "    ('onehot', OneHotEncoder( handle_unknown='ignore', sparse_output=True)),\n",
    "    ('linsvc', LinearSVC(class_weight=class_weights_cv, max_iter=10000, dual=False, verbose=2))\n",
    "])\n",
    "\n",
    "# Set up parameter distributions for XGBoost.\n",
    "param_distributions = {\n",
    "    'linsvc__C': [0.01, 0.1, 1, 2, 4, 8, 10],\n",
    "    'linsvc__loss': ['squared_hinge', 'hinge'],  # 'hinge' is okay too, but 'squared_hinge' usually works better\n",
    "    # 'linsvc__penalty': ['l2'],  # 'l1' can only be used with dual=False and 'liblinear' solver\n",
    "    'linsvc__max_iter': [1000, 5000, 10000],\n",
    "    'linsvc__multi_class': ['ovr', 'crammer_singer'],  # try both strategies\n",
    "}\n",
    "# Perform hyperparameter search using RandomizedSearchCV.\n",
    "randomized_search = RandomizedSearchCV(\n",
    "    estimator=pipeline, \n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=1,\n",
    "    cv=skf, \n",
    "    scoring=balanced_acc_scorer, \n",
    "    verbose=3,\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    "randomized_search.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters:', randomized_search.best_params_)\n",
    "print('Best cross-validation accuracy:', randomized_search.best_score_)\n",
    "\n",
    "cv_scores = cross_val_score(randomized_search.best_estimator_, X_train, y_train, cv=skf, verbose=3, scoring=balanced_acc_scorer)\n",
    "print('Generalization Balanced accuracy (via cross_val_score):', cv_scores.mean())\n",
    "\n",
    "# Make predictions on the test set using the best estimator.\n",
    "test_predictions = randomized_search.predict(X_test)\n",
    "classification_report_with_accuracy_score(y_test, test_predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
