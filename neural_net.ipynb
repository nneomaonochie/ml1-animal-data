{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de625aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nbformat in /opt/anaconda3/lib/python3.12/site-packages (5.10.4)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /opt/anaconda3/lib/python3.12/site-packages (from nbformat) (2.16.2)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /opt/anaconda3/lib/python3.12/site-packages (from nbformat) (4.23.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/anaconda3/lib/python3.12/site-packages (from nbformat) (5.7.2)\n",
      "Requirement already satisfied: traitlets>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from nbformat) (5.14.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=2.6->nbformat) (0.10.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->nbformat) (3.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in /opt/anaconda3/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from xgboost) (1.13.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "dropped columns\n",
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "cleaned breed\n",
      "cleaned color\n",
      "dropped columns\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zg/knpz54sj16988z9vrp2b98fr0000gn/T/ipykernel_73392/850107665.py:4: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dt_series = pd.to_datetime(df['intake_time'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned intake time\n",
      "cleaned intake condition\n",
      "cleaned age and sex\n",
      "cleaned breed\n",
      "cleaned color\n",
      "Done running ml_project.ipynb.\n"
     ]
    }
   ],
   "source": [
    "%pip install nbformat\n",
    "import nbformat\n",
    "\n",
    "# Read your notebook (assuming version 4 for example purposes)\n",
    "nb = nbformat.read(\"ml_project.ipynb\", as_version=4)\n",
    "\n",
    "# Normalize the notebook to add missing id fields and other updates\n",
    "nbformat.validator.validate(nb)\n",
    "\n",
    "# Write the normalized notebook back to a file\n",
    "nbformat.write(nb, \"ml_project_normalized.ipynb\")\n",
    "\n",
    "%run ml_project_normalized.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f3a696c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "\n",
    "def train_sk_classifier(X_train, y_train, X_test):\n",
    "  \"\"\"\n",
    "  Trains an MLP neural network model using a pipeline that includes:\n",
    "  frequency encoding, one-hot encoding, and hyperparameter tuning.\n",
    "\n",
    "  Parameters:\n",
    "      X_train (pd.DataFrame): Training features.\n",
    "      y_train (pd.Series or np.array): Training target values.\n",
    "      X_test (pd.DataFrame): Test features.\n",
    "\n",
    "  Returns:\n",
    "      best_estimator: Best estimator from RandomizedSearchCV.\n",
    "      test_predictions: Predicted labels for X_test.\n",
    "  \"\"\"\n",
    "  \n",
    "  # Construct pipeline:\n",
    "  pipeline = Pipeline([\n",
    "    ('freq', FunctionTransformer(apply_freq_encode, validate=False)),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False)),\n",
    "    ('mlp', MLPClassifier(max_iter=300, random_state=42))\n",
    "  ])\n",
    "\n",
    "  # Define param grid for MLPClassifier\n",
    "  param_distributions = {\n",
    "    'mlp__hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "    'mlp__activation': ['relu', 'tanh'],\n",
    "    'mlp__solver': ['adam'],\n",
    "    'mlp__alpha': [0.0001, 0.001],\n",
    "    'mlp__learning_rate': ['constant', 'adaptive']\n",
    "  }\n",
    "\n",
    "  # Balanced accuracy scorer\n",
    "  balanced_acc = make_scorer(balanced_accuracy_score)\n",
    "\n",
    "  randomized_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    scoring=balanced_acc,\n",
    "    verbose=3,\n",
    "    n_jobs=-1\n",
    "  )\n",
    "\n",
    "  randomized_search.fit(X_train, y_train)\n",
    "\n",
    "  print('Best parameters:', randomized_search.best_params_)\n",
    "  print('Best cross-validation balanced accuracy:', randomized_search.best_score_)\n",
    "\n",
    "  cv_scores = cross_val_score(\n",
    "    randomized_search.best_estimator_,\n",
    "    X_train, y_train,\n",
    "    cv=5,\n",
    "    scoring=balanced_acc,\n",
    "    verbose=3\n",
    "  )\n",
    "  print('Generalization balanced accuracy (via cross_val_score):', cv_scores.mean())\n",
    "\n",
    "  test_predictions = randomized_search.predict(X_test)\n",
    "\n",
    "  # Save results to CSV\n",
    "  df_preds = pd.DataFrame({\n",
    "    'Id': range(1, len(test_predictions) + 1),\n",
    "    'Outcome Type': test_predictions\n",
    "  })\n",
    "  df_preds.to_csv('neural_net_preds.csv', index=False)\n",
    "  print('Predictions saved to neural_net_preds.csv')\n",
    "\n",
    "\n",
    "  return randomized_search.best_estimator_, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63aecae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "  def __init__(self, input_dim, hidden_layers, output_dim, activation='relu'):\n",
    "    super(SimpleMLP, self).__init__()\n",
    "    layers = []\n",
    "    prev_dim = input_dim\n",
    "    for h in hidden_layers:\n",
    "      layers.append(nn.Linear(prev_dim, h))\n",
    "      layers.append(nn.ReLU() if activation == 'relu' else nn.Tanh())\n",
    "      prev_dim = h\n",
    "    layers.append(nn.Linear(prev_dim, output_dim))\n",
    "    self.model = nn.Sequential(*layers)\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.model(x)\n",
    "\n",
    "def train_classifier(X_train, y_train, X_test):\n",
    "  \"\"\"\n",
    "  Trains a PyTorch MLP using frequency + one-hot encoded features.\n",
    "\n",
    "  Parameters:\n",
    "      X_train (pd.DataFrame): Training features.\n",
    "      y_train (np.ndarray): Encoded target labels.\n",
    "      X_test (pd.DataFrame): Test features.\n",
    "\n",
    "  Returns:\n",
    "      best_model: Trained PyTorch model.\n",
    "      test_predictions: Model predictions on test data.\n",
    "  \"\"\"\n",
    "\n",
    "#   # TODO: remove when running on condor\n",
    "#   sample_indices = np.random.choice(len(X_train), size=1000, replace=False)\n",
    "#   X_train = X_train.iloc[sample_indices].reset_index(drop=True)\n",
    "#   y_train = y_train[sample_indices]\n",
    "\n",
    "  X_train_freq = apply_freq_encode(X_train)\n",
    "  X_test_freq = apply_freq_encode(X_test)\n",
    "\n",
    "  # One Hot Encoding\n",
    "  encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "  X_train_encoded = encoder.fit_transform(X_train_freq)\n",
    "  X_test_encoded  = encoder.transform(X_test_freq)\n",
    "\n",
    "  input_dim  = X_train_encoded.shape[1]\n",
    "  output_dim = len(np.unique(y_train))\n",
    "\n",
    "  # Using PyTorch sensors\n",
    "  X_train_tensor = torch.tensor(X_train_encoded, dtype=torch.float32)\n",
    "  y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "  X_test_tensor  = torch.tensor(X_test_encoded, dtype=torch.float32)\n",
    "\n",
    "  dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "  dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "  # Train with random search\n",
    "  param_grid = {\n",
    "    'hidden_layer_sizes': [[50], [100], [100, 50]],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate': [0.01, 0.001],\n",
    "  }\n",
    "\n",
    "  param_list = list(ParameterSampler(param_grid, n_iter=10, random_state=42))\n",
    "\n",
    "  best_score = -1\n",
    "  best_model = None\n",
    "\n",
    "  for params in param_list:\n",
    "    model = SimpleMLP(input_dim, params['hidden_layer_sizes'], output_dim, params['activation'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['alpha'])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(20):\n",
    "      for xb, yb in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Evaluate model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      y_pred = model(X_train_tensor).argmax(dim=1).numpy()\n",
    "      score = balanced_accuracy_score(y_train, y_pred)\n",
    "\n",
    "    if score > best_score:\n",
    "      best_score = score\n",
    "      best_model = model\n",
    "\n",
    "  print(\"Best cross-validation balanced accuracy:\", best_score)\n",
    "\n",
    "  # predict and save \n",
    "  best_model.eval()\n",
    "  with torch.no_grad():\n",
    "    test_predictions = best_model(X_test_tensor).argmax(dim=1).numpy()\n",
    "\n",
    "  df_preds = pd.DataFrame({\n",
    "    'Id': range(1, len(test_predictions) + 1),\n",
    "    'Outcome Type': test_predictions\n",
    "  })\n",
    "  df_preds.to_csv('neural_net_preds.csv', index=False)\n",
    "  print('Predictions saved to neural_net_preds.csv')\n",
    "\n",
    "  return best_model, test_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abe7be4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for Dog data:\n",
      "Best cross-validation balanced accuracy: 1.0\n",
      "Predictions saved to neural_net_preds.csv\n",
      "\n",
      "Training model for Cat data:\n",
      "Best cross-validation balanced accuracy: 1.0\n",
      "Predictions saved to neural_net_preds.csv\n",
      "Combined test predictions saved to: ./test_predictions_combined.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# For Dog:\n",
    "train_dog = df_train[df_train['animal_type'] == 'Dog'].copy()\n",
    "X_train_dog = train_dog.drop(columns=['animal_type', 'outcome_type'])\n",
    "y_train_dog = train_dog['outcome_type']\n",
    "\n",
    "test_dog = df_test[df_test['animal_type'] == 'Dog'].copy()\n",
    "X_test_dog = test_dog.drop(columns=['animal_type'])\n",
    "\n",
    "# For Cat:\n",
    "train_cat = df_train[df_train['animal_type'] == 'Cat'].copy()\n",
    "X_train_cat = train_cat.drop(columns=['animal_type', 'outcome_type'])\n",
    "y_train_cat = train_cat['outcome_type']\n",
    "\n",
    "test_cat = df_test[df_test['animal_type'] == 'Cat'].copy()\n",
    "X_test_cat = test_cat.drop(columns=['animal_type'])\n",
    "\n",
    "## Encode targets with LabelEncoder\n",
    "# Dog encoding\n",
    "le_dog = LabelEncoder()\n",
    "y_train_dog_encoded = le_dog.fit_transform(y_train_dog)\n",
    "\n",
    "# Cat encoding\n",
    "le_cat = LabelEncoder()\n",
    "y_train_cat_encoded = le_cat.fit_transform(y_train_cat)\n",
    "\n",
    "print(\"Training model for Dog data:\")\n",
    "best_estimator_dog, dog_predictions_encoded = train_classifier(X_train_dog, y_train_dog_encoded, X_test_dog)\n",
    "dog_predictions = le_dog.inverse_transform(dog_predictions_encoded)\n",
    "\n",
    "print(\"\\nTraining model for Cat data:\")\n",
    "best_estimator_cat, cat_predictions_encoded = train_classifier(X_train_cat, y_train_cat_encoded, X_test_cat)\n",
    "cat_predictions = le_cat.inverse_transform(cat_predictions_encoded)\n",
    "\n",
    "combine_predictions(dog_predictions, cat_predictions, X_test_dog, X_test_cat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
